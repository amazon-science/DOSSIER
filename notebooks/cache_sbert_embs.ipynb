{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c44de58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from tqdm import trange\n",
    "from fact_check.constants import DATA_DIR, cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79d787b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2700472/1302764665.py:1: DtypeWarning: Columns (4,8,11,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DATA_DIR/'semmeddb/semmeddb_processed_10.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_DIR/'semmeddb/semmeddb_processed_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4ccabc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_predicate(st):\n",
    "    st = st.replace('_', ' ')\n",
    "    if st == \"ISA\":\n",
    "        return 'IS A'\n",
    "    return st\n",
    "\n",
    "df['predicate_clean'] = df['PREDICATE'].apply(clean_predicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04a2db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['OBJECT_NAME'] = df['OBJECT_NAME'].fillna(0.)\n",
    "df['OBJECT_NAME'] = df['OBJECT_NAME'].astype(str)\n",
    "df['SUBJECT_NAME'] = df['SUBJECT_NAME'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "815709c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sent_str'] = df.apply(lambda x: x['SUBJECT_NAME'] + ' ' + x['predicate_clean'] + ' ' + x['OBJECT_NAME'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c041bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def process_batch(sentences_batch):\n",
    "    encoded_input = tokenizer(sentences_batch, padding=True, truncation=True, return_tensors='pt')\n",
    "    for i in encoded_input:\n",
    "        encoded_input[i] = encoded_input[i].to('cuda')    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)    \n",
    "    return sentence_embeddings\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = list(df['sent_str'].values)\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2').to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da2ce6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████| 7060/7060 [14:17<00:00,  8.24it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "embs = []\n",
    "\n",
    "for i in trange(0, len(sentences), batch_size):\n",
    "    batch = sentences[i:i+batch_size]\n",
    "    embs.append(process_batch(batch).detach().cpu().numpy())                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1a7db9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = np.concatenate(embs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bfd49de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7229041, 384)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e77f679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(cache_dir/'kg_embeddings.npy', embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88e23be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
