{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5328376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook will take several hours to run\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "import networkx as nx\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from fact_check.constants import UMLS_DIR, DATA_DIR\n",
    "\n",
    "SEMMEDDB_DIR = DATA_DIR/'semmeddb'\n",
    "\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bc6ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = pd.read_csv(SEMMEDDB_DIR/'semmedVER43_2023_R_PREDICATION.csv' \n",
    "           , encoding='ISO-8859-1',\n",
    "                header = None).drop(columns = [12, 13, 14])\n",
    "\n",
    "g1.columns = ['PREDICATION_ID', 'SENTENCE_ID', 'PMID', 'PREDICATE', 'SUBJECT_CUI', 'SUBJECT_NAME',\n",
    "                 'SUBJECT_SEMTYPE', 'SUBJECT_NOVELTY', 'OBJECT_CUI', 'OBJECT_NAME', 'OBJECT_SEMTYPE',\n",
    "             'OBJECT_NOVELTY']\n",
    "\n",
    "g2 = pd.read_csv(SEMMEDDB_DIR/'semmedVER43_2023_R_PREDICATION_AUX.csv',\n",
    "                 on_bad_lines='skip',\n",
    "                header = None)\n",
    "\n",
    "g2.columns = ['PREDICATION_AUX_ID', 'PREDICATION_ID', 'SUBJECT_TEXT', 'SUBJECT_DIST', 'SUBJECT_MAXDIST',\n",
    "                 'SUBJECT_START_INDEX', 'SUBJECT_END_INDEX', 'SUBJECT_SCORE', 'INDICATOR_TYPE',\n",
    "             'PREDICATE_START_INDEX', 'PREDICATE_END_INDEX', 'OBJECT_TEXT', 'OBJECT_DIST', 'OBJECT_MAXDIST',\n",
    "                 'OBJECT_START_INDEX', 'OBJECT_END_INDEX', 'OBJECT_SCORE', 'CURR_TIMESTAMP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1a47b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = g1.merge(g2, on = 'PREDICATION_ID', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d237a45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "removes = [df['OBJECT_SCORE'].str.isnumeric() == False,\n",
    "           df['SUBJECT_SCORE'].str.isnumeric() == False,\n",
    "#            df['OBJECT_CUI'].str.contains('|', regex = False),\n",
    "#            df['SUBJECT_CUI'].str.contains('|', regex = False),\n",
    "            ~df['OBJECT_CUI'].str.contains('C', regex = False), # will need to remove again after exploding pipes\n",
    "           ~df['SUBJECT_CUI'].str.contains('C', regex = False), # removing only at the end takes up too much memory\n",
    "          ]\n",
    "    \n",
    "df = df[~np.logical_or.reduce(removes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a71626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  keep a minimal set of columns for speed. Most notably drops confidence scores\n",
    "df = df[['PREDICATION_ID', 'PREDICATE', 'SUBJECT_CUI',\n",
    "       'SUBJECT_NAME', 'SUBJECT_SEMTYPE', 'SUBJECT_NOVELTY', 'OBJECT_CUI',\n",
    "       'OBJECT_NAME', 'OBJECT_SEMTYPE', 'OBJECT_NOVELTY']] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fb2582",
   "metadata": {},
   "source": [
    "deal with pipes in object, subject, or both, indicating multiple edges in a single row\n",
    "\n",
    "processing code taken from https://github.com/mmayers12/semmed/blob/master/prepare/01-initial_data_clean.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da3e2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_start = df['SUBJECT_CUI'].str.contains('|', regex=False)\n",
    "multi_end = df['OBJECT_CUI'].str.contains('|', regex=False)\n",
    "pipe_lines = df[multi_start | multi_end]\n",
    "good_lines = df[~multi_start & ~multi_end]\n",
    "\n",
    "multi_start_subset = multi_start[multi_start | multi_end]\n",
    "multi_end_subset = multi_end[multi_start | multi_end]\n",
    "multi_both_subset = multi_start_subset & multi_end_subset\n",
    "\n",
    "start_only_subset = multi_start_subset & ~multi_end_subset\n",
    "end_only_subset = multi_end_subset & ~multi_start_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40931b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipes in subjects\n",
    "start_id_split = pipe_lines.loc[start_only_subset, 'SUBJECT_CUI'].str.split('|')\n",
    "start_name_split = pipe_lines.loc[start_only_subset, 'SUBJECT_NAME'].str.split('|')\n",
    "\n",
    "start_lens = start_id_split.apply(len)\n",
    "all_cols = list(pipe_lines.columns)\n",
    "\n",
    "start_cols = all_cols[:]\n",
    "start_cols.remove('SUBJECT_CUI')\n",
    "start_cols.remove('SUBJECT_NAME')\n",
    "\n",
    "new_starts = dict()\n",
    "for c in start_cols:\n",
    "    tmp = pipe_lines.loc[start_only_subset, c].apply(lambda x: [x]) * start_lens\n",
    "    new_starts[c] = [x for x in chain(*tmp.values)]\n",
    "fixed_starts = pd.DataFrame(new_starts)\n",
    "fixed_starts['SUBJECT_CUI'] = [x for x in chain(*start_id_split.values)]\n",
    "fixed_starts['SUBJECT_NAME'] = [x for x in chain(*start_name_split.values)]\n",
    "\n",
    "fixed_starts = fixed_starts[all_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528ff086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipes in objects\n",
    "end_id_split = pipe_lines.loc[end_only_subset, 'OBJECT_CUI'].str.split('|')\n",
    "end_name_split = pipe_lines.loc[end_only_subset, 'OBJECT_NAME'].str.split('|')\n",
    "\n",
    "end_lens = end_id_split.apply(len)\n",
    "end_lens1 = end_name_split.apply(len)\n",
    "bad_lines = pipe_lines.loc[end_only_subset][(end_lens != end_lens1)].index\n",
    "\n",
    "pipe_lines = pipe_lines.drop(bad_lines) # bad lines where number of pipes are different across ID and name\n",
    "end_only_subset = end_only_subset.drop(bad_lines)\n",
    "multi_both_subset = multi_both_subset.drop(bad_lines)\n",
    "\n",
    "end_id_split = pipe_lines.loc[end_only_subset, 'OBJECT_CUI'].str.split('|')\n",
    "end_name_split = pipe_lines.loc[end_only_subset, 'OBJECT_NAME'].str.split('|')\n",
    "end_lens = end_id_split.apply(len)\n",
    "\n",
    "end_cols = all_cols[:]\n",
    "end_cols.remove('OBJECT_CUI')\n",
    "end_cols.remove('OBJECT_NAME')\n",
    "\n",
    "new_ends = dict()\n",
    "for c in end_cols:\n",
    "    tmp = pipe_lines.loc[end_only_subset, c].apply(lambda x: [x]) * end_lens\n",
    "    new_ends[c] = [x for x in chain(*tmp.values)]\n",
    "    \n",
    "fixed_ends = pd.DataFrame(new_ends)\n",
    "fixed_ends['OBJECT_CUI'] = [x for x in chain(*end_id_split.values)]\n",
    "fixed_ends['OBJECT_NAME'] = [x for x in chain(*end_name_split.values)]\n",
    "\n",
    "fixed_ends = fixed_ends[all_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d99fbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipes in both subject and object\n",
    "start_id_split = pipe_lines.loc[multi_both_subset, 'SUBJECT_CUI'].str.split('|')\n",
    "start_name_split = pipe_lines.loc[multi_both_subset, 'SUBJECT_NAME'].str.split('|')\n",
    "start_lens = start_id_split.apply(len)\n",
    "\n",
    "end_id_split = pipe_lines.loc[multi_both_subset, 'OBJECT_CUI'].str.split('|')\n",
    "end_name_split = pipe_lines.loc[multi_both_subset, 'OBJECT_NAME'].str.split('|')\n",
    "end_lens = end_id_split.apply(len)\n",
    "\n",
    "start_id_split = start_id_split * end_lens\n",
    "start_name_split = start_name_split * end_lens\n",
    "\n",
    "end_id_split = end_id_split * start_lens\n",
    "end_name_split = end_name_split * start_lens\n",
    "\n",
    "sorting_df = pd.DataFrame()\n",
    "sorting_df['ID'] = start_id_split\n",
    "sorting_df['NAME'] = start_name_split\n",
    "\n",
    "sorted_start_id_split = sorting_df['ID'].apply(lambda x: sorted(x))\n",
    "sorted_start_name_split = sorting_df.apply(lambda row: [x for y,x in sorted(zip(row['ID'], row['NAME']))], axis = 1)\n",
    "\n",
    "both_cols = all_cols[:]\n",
    "both_cols.remove('SUBJECT_CUI')\n",
    "both_cols.remove('SUBJECT_NAME')\n",
    "both_cols.remove('OBJECT_CUI')\n",
    "both_cols.remove('OBJECT_NAME')\n",
    "\n",
    "new_both = dict()\n",
    "for c in both_cols:\n",
    "    tmp = pipe_lines.loc[multi_both_subset, c].apply(lambda x: [x]) * (start_lens * end_lens)\n",
    "    new_both[c] = [x for x in chain(*tmp.values)]\n",
    "    \n",
    "fixed_both = pd.DataFrame(new_both)\n",
    "\n",
    "fixed_both['SUBJECT_CUI'] = [x for x in chain(*sorted_start_id_split.values)]\n",
    "fixed_both['SUBJECT_NAME'] = [x for x in chain(*sorted_start_name_split.values)]\n",
    "\n",
    "fixed_both['OBJECT_CUI'] = [x for x in chain(*end_id_split.values)]\n",
    "fixed_both['OBJECT_NAME'] = [x for x in chain(*end_name_split.values)]\n",
    "\n",
    "fixed_both = fixed_both[all_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d48a053",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([good_lines, fixed_starts, fixed_ends, fixed_both]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02eca7b",
   "metadata": {},
   "source": [
    "Continuing on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2725b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "removes = [ ~df['OBJECT_CUI'].str.contains('C', regex = False),\n",
    "           ~df['SUBJECT_CUI'].str.contains('C', regex = False),\n",
    "          ]\n",
    "    \n",
    "df = df[~np.logical_or.reduce(removes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c0b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_refs = df.groupby(['SUBJECT_CUI', 'OBJECT_CUI', 'PREDICATE']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4e4d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633621e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_rows = num_refs[num_refs >= cutoff]\n",
    "df_sel = (df.merge(select_rows.to_frame().rename(columns = {0: 'n_refs'}).reset_index(),\n",
    "                 on = ['SUBJECT_CUI', 'OBJECT_CUI', 'PREDICATE'])\n",
    "         .drop_duplicates(subset = ['SUBJECT_CUI', 'OBJECT_CUI', 'PREDICATE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c3ddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (SEMMEDDB_DIR/'sem_types.csv').is_file():\n",
    "    sem_types = pd.read_csv(SEMMEDDB_DIR/'sem_types.csv', sep = '|').set_index('abbv')['sem_type'].to_dict()\n",
    "    df_sel['SUBJECT_SEMTYPE_STR'] = df_sel['SUBJECT_SEMTYPE'].map(sem_types)\n",
    "    df_sel['OBJECT_SEMTYPE_STR'] = df_sel['OBJECT_SEMTYPE'].map(sem_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33eb11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['aapp', 'gngm', 'celf', 'moft', 'genf']\n",
    "df_sel = df_sel[(~df_sel['OBJECT_SEMTYPE'].isin(to_drop)) & (~df_sel['SUBJECT_SEMTYPE'].isin(to_drop))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c47b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sel.shape # (874459, 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5c5075",
   "metadata": {},
   "source": [
    "## Add in SNOMED hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f1ae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "hier = pd.read_csv(UMLS_DIR/'MRHIER.RRF', sep = '|', header = None)\n",
    "hier = hier[hier[4] == 'SNOMEDCT_US'].dropna(subset = [5])\n",
    "\n",
    "atom_cui_mapping = pd.read_csv(UMLS_DIR/'MRCONSO.RRF', sep = '|', header = None)\n",
    "atom_cui_mapping = atom_cui_mapping[atom_cui_mapping[11] == \"SNOMEDCT_US\"][[0, 7, 14]]\n",
    "atom_cui_mapping.columns = ['cui', 'atom', 'str_label']\n",
    "\n",
    "hier['atom_list'] = hier[6].apply(lambda x: x.split('.'))\n",
    "\n",
    "exploded_hier = hier[[0, 'atom_list']].explode('atom_list')\n",
    "\n",
    "merged_df = exploded_hier.rename(columns = {0: 'SUBJECT_CUI', 'atom_list': 'atom'}).merge(atom_cui_mapping, on = 'atom')\n",
    "\n",
    "merged_df = merged_df.rename(columns = {'cui': 'OBJECT_CUI'}).drop_duplicates(subset = ['SUBJECT_CUI', 'OBJECT_CUI'])\n",
    "merged_df['PREDICATE'] = 'ISA'\n",
    "merged_df = merged_df[~(merged_df.OBJECT_CUI.isin(['C2720507', 'C0037088']))] # generic concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d836fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_rows = [('C0428881', 'C0871470'), # Non-invasive systolic arterial pressure\n",
    "            ('C0428881', 'C1306620'),\n",
    "           ('C0428884', 'C0428883'), # Non-invasive diastolic arterial pressure\n",
    "            ('C0428884', 'C1305849')]\n",
    "add_rows_df = pd.DataFrame(add_rows, columns = ['SUBJECT_CUI', 'OBJECT_CUI']).assign(PREDICATE = 'ISA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd6d1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat((merged_df, add_rows_df), ignore_index = True).drop_duplicates(subset = ['SUBJECT_CUI', 'OBJECT_CUI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe36c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the str names for each of the CUIs. This is approximate since we are taking the name of the atoms.\n",
    "name_mapping = (atom_cui_mapping[['cui', 'str_label']].groupby('cui').agg({'str_label': 'first'})\n",
    "                ['str_label'].to_dict()\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d7a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop(columns = ['atom', 'str_label']).assign(\n",
    "    SUBJECT_NAME = merged_df['SUBJECT_CUI'].map(name_mapping),\n",
    "    OBJECT_NAME = merged_df['OBJECT_CUI'].map(name_mapping),\n",
    "    SUBJECT_NOVELTY = 1,\n",
    "    OBJECT_NOVELTY = 1,\n",
    "    PREDICATION_ID = np.arange(df_sel['PREDICATION_ID'].max() + 1, df_sel['PREDICATION_ID'].max() + 1 + len(merged_df))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b2d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat((df_sel, merged_df), ignore_index = True).drop_duplicates(subset = ['SUBJECT_CUI', 'OBJECT_CUI', 'PREDICATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4501ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.reset_index(drop = True).to_csv(SEMMEDDB_DIR/f'semmeddb_processed_{cutoff}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896237bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final # 7229042 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2af888e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
